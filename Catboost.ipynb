{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,precision_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,auc, confusion_matrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=pd.read_csv(\"data/test.csv\")\n",
    "data_train=pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard','IsActiveMember', 'EstimatedSalary', 'Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Age']\n",
    "\n",
    "# Get unique values for each specified column\n",
    "for column in columns_of_interest:\n",
    "    unique_values = data_train[column].unique()\n",
    "    print(f\"Unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enlever les variables qui ne sont pas nécessaires\n",
    "data_train = data_train.drop(columns=[\"id\",\"CustomerId\", \"Surname\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns=[\"id\",\"CustomerId\", \"Surname\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data_train[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, cmap='Greens',linewidths=1,mask=np.triu(corr_matrix),fmt = '.2f', annot=True)\n",
    "plt.title('Matrice de Corrélation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les variables les plus corrélées avec une variable cible\n",
    "top_corr_with_target = corr_matrix[\"Exited\"].sort_values(ascending=False)\n",
    "top_corr_with_target= pd.DataFrame(top_corr_with_target)\n",
    "top_corr_with_target[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier s'il y a des doublons\n",
    "doublons = data_train.duplicated()\n",
    "nb_doublons = doublons.sum()\n",
    "print(f\"Nombre de doublons : {nb_doublons}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_na = data_train.isna().any().any()\n",
    "print(\"Any NA values in the dataset:\", has_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous divisons notre base de données en ensembles d'entraînement (train) et de test avant de réaliser toutes les manipulations, afin d'éviter de biaiser les résultats sur l'ensemble de test.\n",
    "(page 274 cours ML: Model Selection (hold out a validation set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selection, test_selection = train_test_split(data_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¨Pour set d'apprentissage\n",
    "for col in numeric_cols:\n",
    "\n",
    "    Q1 = train_selection[col].quantile(0.25)\n",
    "    Q3 = train_selection[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    number_outliers = train_selection[(train_selection[col] < lower_bound) | (train_selection[col] > upper_bound)].shape[0]\n",
    "\n",
    "    print(\"Le nombre des outliers de la variable\", col, \"est\", number_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Exited\", y = \"CreditScore\", data=train_selection)\n",
    "plt.title('CreditScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = train_selection[\"CreditScore\"].quantile(0.25)\n",
    "Q3 = train_selection[\"CreditScore\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = train_selection[(train_selection[\"CreditScore\"] < lower_bound) | (train_selection[\"CreditScore\"] > upper_bound)]\n",
    "print(f\"Outliers for CreditScore:\")\n",
    "print(outliers[[\"CreditScore\"]])  # Print only the specific column for clarity\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that credit scores typically don't exceed 850, the value 4818 is clearly an anomaly => point abberant.\n",
    "\n",
    "Scores in the 431–439 range are typically valid but represent very poor creditworthiness. They should usually be kept in the dataset unless there is a specific reason to filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des valeurs aberrantes pour CreditScore\n",
    "train_selection = train_selection[train_selection['CreditScore'] <= 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For test set\n",
    "X_test = X_test[X_test['CreditScore'] <= 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¨Pour set de validation\n",
    "Q1 = test_selection[\"CreditScore\"].quantile(0.25)\n",
    "Q3 = test_selection[\"CreditScore\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = test_selection[(test_selection[\"CreditScore\"] < lower_bound) | (test_selection[\"CreditScore\"] > upper_bound)]\n",
    "print(f\"Outliers for CreditScore:\")\n",
    "print(outliers[[\"CreditScore\"]])  # Print only the specific column for clarity\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")  # Separator for readability\n",
    "\n",
    "#On voit pas de probleme ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodez les variables catégorielles en utilisant One-Hot Encoding\n",
    "categorical_columns = [col for col in ['Geography', 'Gender'] if col in train_selection.columns]\n",
    "train_selection = pd.get_dummies(train_selection, columns=categorical_columns, drop_first=True)\n",
    "test_selection = pd.get_dummies(test_selection, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "binary_columns = ['Geography_Germany','Geography_Spain', 'Gender_Male']\n",
    "train_selection[binary_columns] = train_selection[binary_columns].astype(int)\n",
    "test_selection[binary_columns] = test_selection[binary_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodez les variables catégorielles en utilisant One-Hot Encoding\n",
    "categorical_columns = [col for col in ['Geography', 'Gender'] if col in X_test.columns]\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "X_test[binary_columns] = X_test[binary_columns].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_selection.drop(columns=['Exited'])  # Toutes les colonnes sauf 'Exited' pour les variables explicatives\n",
    "y_train = train_selection['Exited']                 # Cible (1 = churn, 0 = non-churn)\n",
    "\n",
    "X_val = test_selection.drop(columns=['Exited'])  # Toutes les colonnes sauf 'Exited' pour les variables explicatives\n",
    "y_val = test_selection['Exited'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "continuous_columns = (X_train.drop(columns=['Geography_Germany','Geography_Spain', 'Gender_Male'])).columns\n",
    "binary_columns = ['Geography_Germany','Geography_Spain', 'Gender_Male']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# On applique le scaler aux colonnes continues de X_train et X_test\n",
    "X_train_scaled = scaler.fit_transform(X_train[continuous_columns])\n",
    "X_val_scaled = scaler.transform(X_val[continuous_columns])\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=continuous_columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=continuous_columns, index=X_val.index)\n",
    "\n",
    "# On concatène les colonnes binaires avec les colonnes continues mises à l'échelle\n",
    "X_train_scaled = pd.concat([X_train_scaled, X_train[binary_columns]], axis=1)\n",
    "X_val_scaled = pd.concat([X_val_scaled, X_val[binary_columns]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = (X_test.drop(columns=['Geography_Germany','Geography_Spain', 'Gender_Male'])).columns\n",
    "binary_columns = ['Geography_Germany','Geography_Spain', 'Gender_Male']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# On applique le scaler aux colonnes continues de X_train et X_test\n",
    "X_test_scaled = scaler.fit_transform(X_test[continuous_columns])\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=continuous_columns, index=X_test.index)\n",
    "\n",
    "# On concatène les colonnes binaires avec les colonnes continues mises à l'échelle\n",
    "X_test_scaled = pd.concat([X_test_scaled, X_test[binary_columns]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calcul_vif(X):\n",
    "    \"\"\"\n",
    "    Calcul du VIF (Variance Inflation Factor) pour chaque variable dans X.\n",
    "    \"\"\"\n",
    "    donnees_vif = pd.DataFrame()\n",
    "    donnees_vif[\"feature\"] = X.columns\n",
    "    donnees_vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return donnees_vif\n",
    "\n",
    "def calcul_correlation(X, y):\n",
    "    \"\"\"\n",
    "    Calcul de la corrélation entre chaque variable de X et la variable cible y.\n",
    "    \"\"\"\n",
    "    donnees_correlation = X.apply(lambda x: x.corr(y))\n",
    "    donnees_correlation = donnees_correlation.reset_index()\n",
    "    donnees_correlation.columns = [\"feature\", \"correlation\"]\n",
    "    return donnees_correlation\n",
    "\n",
    "def remove_highvif_lowcorrelation(X, y, seuil_vif=5):\n",
    "    \"\"\"\n",
    "    Suppression des variables avec un VIF élevé (VIF>5) et une corrélation faible avec la cible y \n",
    "    jusqu'à ce que toutes les variables restantes aient un VIF inférieur au seuil.\n",
    "    \"\"\"\n",
    "    # Calcul du VIF et de la corrélation\n",
    "    donnees_vif = calcul_vif(X)\n",
    "    donnees_correlation = calcul_correlation(X, y)\n",
    "    \n",
    "    # Fusionner les résultats de VIF et des corrélations\n",
    "    donnees_combinees = pd.merge(donnees_vif, donnees_correlation, on=\"feature\")\n",
    "\n",
    "    while donnees_combinees['VIF'].max() > seuil_vif:\n",
    "        # Trier par VIF élevé et corrélation faible\n",
    "        donnees_combinees = donnees_combinees.sort_values(by=['VIF', 'correlation'], ascending=[False, True])\n",
    "        var_a_eliminer = donnees_combinees.iloc[0]['feature']  # La variable à supprimer\n",
    "        print(f\"On élimine '{var_a_eliminer}' avec un VIF de: {donnees_combinees.iloc[0]['VIF']} et une corrélation de: {donnees_combinees.iloc[0]['correlation']}\")\n",
    "        \n",
    "        # Suppression de la variable dans X\n",
    "        X = X.drop(columns=[var_a_eliminer])\n",
    "\n",
    "        # Recalculer VIF et corrélation après suppression\n",
    "        donnees_vif = calcul_vif(X)\n",
    "        donnees_correlation = calcul_correlation(X, y)\n",
    "        donnees_combinees = pd.merge(donnees_vif, donnees_correlation, on=\"feature\") \n",
    "\n",
    "    return X, donnees_combinees\n",
    "\n",
    "X_train_scaled, vif_correlation_data = remove_highvif_lowcorrelation(X_train_scaled, y_train, seuil_vif=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled=X_val_scaled[X_train_scaled.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Parameter grid for CatBoost\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],\n",
    "    'depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bylevel': [0.8, 1.0],\n",
    "    'scale_pos_weight': [1, 5, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 10],\n",
    "    'bagging_temperature': [0.5, 1, 2, 5]\n",
    "}\n",
    "\n",
    "# Cross-validation settings\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# CatBoost model\n",
    "model = CatBoostClassifier(boosting_type='Plain', eval_metric='AUC', verbose=0)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_proba_best = best_model.predict_proba(X_val)[:, 1]\n",
    "auc_best = roc_auc_score(y_val, y_pred_proba_best)\n",
    "print(\"Best AUC after hyperparameter tuning:\", auc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Créez sample_submission avec 'id' et 'Churn_Probability'\n",
    "sample_submission_test_N = data_test[['id']].copy()  # Copie uniquement la colonne 'id' de data_test\n",
    "sample_submission_test_N[\"Exited\"] = y_pred_proba_best  # Ajout des probabilités de churn\n",
    "\n",
    "# Optionnel : Exportez sample_submission en CSV\n",
    "sample_submission_test_N.to_csv(\"sample_submission_test_catboost.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
